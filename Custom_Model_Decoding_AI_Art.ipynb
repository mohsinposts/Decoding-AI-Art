{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b56c2c-ca28-4323-b882-2e4657c34527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import transformers\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as vison_functions\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4337bd-be21-4fbd-ab4f-e5e1477f5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is available then use GPU\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e072954-8412-4693-b4b8-42fb96889db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BERT tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
    "\n",
    "# specify hyper parameters\n",
    "max_token_length = 20\n",
    "vocab_size = 30522\n",
    "batch_size = 20\n",
    "embed_size= 512\n",
    "hidden_size= 512\n",
    "epochs = 0\n",
    "learning_rate = 0.00000001\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0430cd-26b8-4ee5-b2d1-66057aafbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path so it's easier to load model later\n",
    "saved_model_path = \"saved_model.pt\"\n",
    "\n",
    "# specify path to custom dataset\n",
    "data_dir = '../data/DecodingAI'\n",
    "image_dir = f'{data_dir}/images'\n",
    "csv_file = f'{data_dir}/dataset.csv'\n",
    "\n",
    "# create a pandas dataframe from imported dataset\n",
    "df = pd.read_csv(csv_file,  delimiter=\"~\", names=['image', 'caption'], header=None)\n",
    "df['image'] = image_dir+'/'+df['image']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a3f43-873c-4e36-8183-6c26f6602f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain Train Test Split \n",
    "train, test = train_test_split(df, test_size=0.01, random_state=1234) #change to 0.2\n",
    "\n",
    "## Reset Indexes \n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "## Obtain Validation and Test Split \n",
    "val, test = train_test_split(test, test_size=0.01, random_state=1234) #change to 0.5\n",
    "\n",
    "## Reset Indexes \n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "## Let's see how many entries we have\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca2df4-ff5e-4f3a-be29-64bbc884dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehtod allows us to resize all images to be the same dimention while aslo not losing any detail\n",
    "# since we are not cropping images, instead rezining and then adding padding if demintions are smaller\n",
    "# than preferred dimentions\n",
    "class ImgSquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return vison_functions.pad(image, padding, 0, 'constant')\n",
    "\n",
    "# Resizes and normalizes images\n",
    "def ResizeAndNormalize(image):\n",
    "    transform = transforms.Compose([\n",
    "                ImgSquarePad(),\n",
    "                transforms.Resize(300),                          # smaller edge of image resized to 325\n",
    "                transforms.CenterCrop(300),                      # get 224x224 crop from random location\n",
    "                transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                                     (0.229, 0.224, 0.225))])\n",
    "    return transform(image)\n",
    "\n",
    "# Creates image caption pairs, while also encoding the captions with BERT\n",
    "class AIDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, train):\n",
    "        self.df = df\n",
    "        self.captions = self.df['caption']\n",
    "        self.images = self.df['image']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.captions[index]\n",
    "        image_name = self.images[index]\n",
    "        image = Image.open(image_name).convert(\"RGB\")\n",
    "        image = ResizeAndNormalize(image)\n",
    "        encoded_labels = tokenizer(label, \n",
    "                              return_token_type_ids = False, \n",
    "                              return_attention_mask = False, \n",
    "                              max_length = max_token_length, \n",
    "                              padding = \"max_length\",\n",
    "                              truncation = True,\n",
    "                              return_tensors = \"pt\")\n",
    "        return image.to(device),  encoded_labels[\"input_ids\"].flatten().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede79367-fa82-4811-92c1-a39961690b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies class defined above to train, val, and test dataset and then adds them to the pytorch dataloader\n",
    "train_dataset = AIDataset(train, True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, drop_last=True)\n",
    "\n",
    "val_dataset = AIDataset(val, False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size,drop_last=True)\n",
    "\n",
    "test_dataset = AIDataset(test, False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c39fd2-22bc-474f-9b2a-2cbaedd7e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image from training data\n",
    "def imshow(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# get some random training images and captions from the train data loader\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show image and cpation\n",
    "imshow(torchvision.utils.make_grid(images[3]))\n",
    "original_tokens = tokenizer.decode(labels[3])\n",
    "print(original_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa574a-8ad7-4e07-8e0a-2f1e21ec334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the EfficientNetB3 pretrained CNN, but we are finetuning the last two layers\n",
    "# which are the linear layer and the dropout layer\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        model = efficientnet_b3(weights=EfficientNet_B3_Weights.DEFAULT)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(model.children())[:-1]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.embed = nn.Linear(model.classifier[1].in_features, embed_size)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.dropout(features)\n",
    "        features = self.embed(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d1746c-b81d-4fa9-9ae5-4ec92f436652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using a custom LSTM with 512 hidden size\n",
    "# note: this LSTM is not bidirectional\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, labels):\n",
    "        features = features.unsqueeze(1)\n",
    "        embed = self.embedding(labels)\n",
    "        embed = torch.cat((features,embed), dim=1)\n",
    "        lstm_out, _ = self.lstm(embed)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        outputs = outputs.view(-1, self.vocab_size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86b2831-fad2-4364-be21-c365b4d9d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class connects the output of the CNN (ecnoder) into the input of the LSTM (decoder) we are using\n",
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = CNN(embed_size)\n",
    "        self.decoder = RNN(embed_size, hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, images, labels):\n",
    "        features = self.encoder(images)\n",
    "        out = self.decoder(features, labels)\n",
    "        return out\n",
    "    \n",
    "    # Class is used to get promt from image after model is trained\n",
    "    def captionImage(self, image, max_token_length):\n",
    "        with torch.no_grad():\n",
    "            token_ids = []\n",
    "            image = ResizeAndNormalize(image).unsqueeze(0).to(device)\n",
    "            x = self.encoder(image)\n",
    "            h0c0 = None\n",
    "            for _ in range(max_token_length):\n",
    "                hiddens, h0c0 = self.decoder.lstm(x, h0c0)\n",
    "                output = self.decoder.fc(hiddens)\n",
    "                predicted = output.argmax(1)\n",
    "                token_ids.append(predicted)\n",
    "                x = self.decoder.embedding(predicted)\n",
    "            token_ids = torch.stack(token_ids)\n",
    "            token_ids = token_ids.view(-1)\n",
    "            return tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6761a6bc-33fa-4ec4-85c9-08ef8cc0b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop used for while training model only\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "       \n",
    "        # Forward\n",
    "        pred = model(images, labels[:, :-1])\n",
    "        loss = loss_fn(pred, labels.view(-1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print Progress\n",
    "        if (batch+1) % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(images)\n",
    "            print(f\"Progress:[{current:>5d}/{size:>5d}] loss:{loss:>10f}\")\n",
    "            \n",
    "    # same model after each batch is trained       \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, saved_model_path)\n",
    "\n",
    "# Loop used for validation and test dataset\n",
    "def test_loop(dataloader, model, loss_fn, dataloader_type):\n",
    "    model.eval()\n",
    "    # Initilialize vairables\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            # Send data to same device as model\n",
    "            pred = model(images, labels)\n",
    "            \n",
    "            # Accumalate accuracy and loss\n",
    "            test_loss += loss_fn(pred, labels.view(-1)).item()\n",
    "            correct += (pred.argmax(1) == labels.view(-1)).type(torch.float).sum().item()\n",
    "\n",
    "    # Normalize accuracy and loss\n",
    "    test_loss /= num_batches\n",
    "    correct /= size * max_token_length\n",
    "\n",
    "    # Print test accuracy and averge loss\n",
    "    print(f\"{dataloader_type} Error: Accuracy:{(100*correct):>0.1f}%, Avg loss:{test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c0692fd-661a-42fa-a268-c74f83a5e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# specify model, loss function, & optimizer\n",
    "model = CNN2RNN(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# get saved model before training\n",
    "if load_model: \n",
    "    checkpoint = torch.load(saved_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "# start training model\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn, \"Val\")\n",
    "    \n",
    "# test model metrics on test dataset\n",
    "test_loop(test_dataloader, encoder, decoder, loss_fn, \"Test\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1beb13-c713-407a-be87-bf221a575414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use model: get a prompt given AI generated image\n",
    "image1 = Image.open('img11.jpg')\n",
    "img = ResizeAndNormalize(image1).to(device)\n",
    "imshow(torchvision.utils.make_grid(img))\n",
    "pred = model.captionImage(image1, max_token_length)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "pytorchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
